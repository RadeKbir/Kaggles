{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Santander Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " team members : Mohamed Reda BRIK / Mohamed AL ANI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import timeit\n",
    "import warnings\n",
    "from sklearn import preprocessing\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_raw = pd.read_csv(\"C:\\\\Users\\Mohamed\\Kaggle\\\\Santander\\\\train_ver2.csv\", chunksize=100000, header=0)\n",
    "test_raw = pd.read_csv(\"C:\\\\Users\\Mohamed\\Kaggle\\\\Santander\\\\test_ver2.csv\", chunksize=10000, header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data size reducing\n",
    "\n",
    "(credits to Jared Turkewitz : https://www.kaggle.com/jturkewitz/santander-product-recommendation/reduce-size-of-dataset-to-1-gb/notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_cols = ['ind_ahor_fin_ult1','ind_aval_fin_ult1','ind_cco_fin_ult1',\n",
    "               'ind_cder_fin_ult1','ind_cno_fin_ult1','ind_ctju_fin_ult1',\n",
    "               'ind_ctma_fin_ult1','ind_ctop_fin_ult1','ind_ctpp_fin_ult1',\n",
    "               'ind_deco_fin_ult1','ind_deme_fin_ult1','ind_dela_fin_ult1',\n",
    "               'ind_ecue_fin_ult1','ind_fond_fin_ult1','ind_hip_fin_ult1',\n",
    "               'ind_plan_fin_ult1','ind_pres_fin_ult1','ind_reca_fin_ult1',\n",
    "               'ind_tjcr_fin_ult1','ind_valo_fin_ult1','ind_viv_fin_ult1',\n",
    "               'ind_nomina_ult1','ind_nom_pens_ult1','ind_recibo_ult1']\n",
    "\n",
    "canal_dict = {'KAI': 35,'KBG': 17,'KGU': 149,'KDE': 47,'KAJ': 41,'KCG': 59,\n",
    " 'KHM': 12,'KAL': 74,'KFH': 140,'KCT': 112,'KBJ': 133,'KBL': 88,'KHQ': 157,'KFB': 146,'KFV': 48,'KFC': 4,\n",
    " 'KCK': 52,'KAN': 110,'KES': 68,'KCB': 78,'KBS': 118,'KDP': 103,'KDD': 113,'KBX': 116,'KCM': 82,\n",
    " 'KAE': 30,'KAB': 28,'KFG': 27,'KDA': 63,'KBV': 100,'KBD': 109,'KBW': 114,'KGN': 11,\n",
    " 'KCP': 129,'KAK': 51,'KAR': 32,'KHK': 10,'KDS': 124,'KEY': 93,'KFU': 36,'KBY': 111,\n",
    " 'KEK': 145,'KCX': 120,'KDQ': 80,'K00': 50,'KCC': 29,'KCN': 81,'KDZ': 99,'KDR': 56,\n",
    " 'KBE': 119,'KFN': 42,'KEC': 66,'KDM': 130,'KBP': 121,'KAU': 142,'KDU': 79,\n",
    " 'KCH': 84,'KHF': 19,'KCR': 153,'KBH': 90,'KEA': 89,'KEM': 155,'KGY': 44,'KBM': 135,\n",
    " 'KEW': 98,'KDB': 117,'KHD': 2,'RED': 8,'KBN': 122,'KDY': 61,'KDI': 150,'KEU': 72,\n",
    " 'KCA': 73,'KAH': 31,'KAO': 94,'KAZ': 7,'004': 83,'KEJ': 95,'KBQ': 62,'KEZ': 108,\n",
    " 'KCI': 65,'KGW': 147,'KFJ': 33,'KCF': 105,'KFT': 92,'KED': 143,'KAT': 5,'KDL': 158,\n",
    " 'KFA': 3,'KCO': 104,'KEO': 96,'KBZ': 67,'KHA': 22,'KDX': 69,'KDO': 60,'KAF': 23,'KAW': 76,\n",
    " 'KAG': 26,'KAM': 107,'KEL': 125,'KEH': 15,'KAQ': 37,'KFD': 25,'KEQ': 138,'KEN': 137,\n",
    " 'KFS': 38,'KBB': 131,'KCE': 86,'KAP': 46,'KAC': 57,'KBO': 64,'KHR': 161,'KFF': 45,\n",
    " 'KEE': 152,'KHL': 0,'007': 71,'KDG': 126,'025': 159,'KGX': 24,'KEI': 97,'KBF': 102,\n",
    " 'KEG': 136,'KFP': 40,'KDF': 127,'KCJ': 156,'KFR': 144,'KDW': 132,-1: 6,'KAD': 16,\n",
    " 'KBU': 55,'KCU': 115,'KAA': 39,'KEF': 128,'KAY': 54,'KGC': 18,'KAV': 139,'KDN': 151,\n",
    " 'KCV': 106,'KCL': 53,'013': 49,'KDV': 91,'KFE': 148,'KCQ': 154,'KDH': 14,'KHN': 21,\n",
    " 'KDT': 58,'KBR': 101,'KEB': 123,'KAS': 70,'KCD': 85,'KFL': 34,'KCS': 77,'KHO': 13,\n",
    " 'KEV': 87,'KHE': 1,'KHC': 9,'KFK': 20,'KDC': 75,'KFM': 141,'KHP': 160,'KHS': 162,\n",
    " 'KFI': 134,'KGV': 43}\n",
    "\n",
    "\n",
    "pais_dict = {'LV': 102,'CA': 2,'GB': 9,'EC': 19,'BY': 64,'ML': 104,'MT': 118,\n",
    " 'LU': 59,'GR': 39,'NI': 33,'BZ': 113,'QA': 58,'DE': 10,'AU': 63,'IN': 31,\n",
    " 'GN': 98,'KE': 65,'HN': 22,'JM': 116,'SV': 53,'TH': 79,'IE': 5,'TN': 85,\n",
    " 'PH': 91,'ET': 54,'AR': 13,'KR': 87,'GA': 45,'FR': 8,'SG': 66,'LB': 81,\n",
    " 'MA': 38,'NZ': 93,'SK': 69,'CN': 28,'GI': 96,'PY': 51,'SA': 56,'PL': 30,\n",
    " 'PE': 20,'GE': 78,'HR': 67,'CD': 112,'MM': 94,'MR': 48,'NG': 83,'HU': 106,\n",
    " 'AO': 71,'NL': 7,'GM': 110,'DJ': 115,'ZA': 75,'OM': 100,'LT': 103,'MZ': 27,\n",
    " 'VE': 14,'EE': 52,'CF': 109,'CL': 4,'SL': 97,'DO': 11,'PT': 26,'ES': 0,\n",
    " 'CZ': 36,'AD': 35,'RO': 41,'TW': 29,'BA': 61,'IS': 107,'AT': 6,'ZW': 114,\n",
    " 'TR': 70,'CO': 21,'PK': 84,'SE': 24,'AL': 25,'CU': 72,'UY': 77,'EG': 74,'CR': 32,\n",
    " 'GQ': 73,'MK': 105,'KW': 92,'GT': 44,'CM': 55,'SN': 47,'KZ': 111,'DK': 76,\n",
    " 'LY': 108,'AE': 37,'PA': 60,'UA': 49,'GW': 99,'TG': 86,'MX': 16,'KH': 95,\n",
    " 'FI': 23,'NO': 46,'IT': 18,'GH': 88, 'JP': 82,'RU': 43,'PR': 40,'RS': 89,\n",
    " 'DZ': 80,'MD': 68,-1: 1,'BG': 50,'CI': 57,'IL': 42,'VN': 90,'CH': 3,'US': 15,'HK': 34,\n",
    " 'CG': 101,'BO': 62,'BR': 17,'BE': 12,'BM': 117}\n",
    "\n",
    "emp_dict = {'N':0,-1:-1,'A':1,'B':2,'F':3,'S':4}\n",
    "indfall_dict = {'N':0,-1:-1,'S':1}\n",
    "sexo_dict = {'V':0,'H':1,-1:-1}\n",
    "tiprel_dict = {'A':0,-1:-1,'I':1,'P':2,'N':3,'R':4}\n",
    "indresi_dict = {'N':0,-1:-1,'S':1}\n",
    "indext_dict = {'N':0,-1:-1,'S':1}\n",
    "conyuemp_dict = {'N':0,-1:-1,'S':1}\n",
    "segmento_dict = {-1:-1,'01 - TOP':1,'02 - PARTICULARES':2,'03 - UNIVERSITARIO':3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tic=timeit.default_timer()\n",
    "def resize_data(DF,is_DF=True):\n",
    "    DF.replace(' NA', -2, inplace=True)\n",
    "    DF.replace('         NA', -2, inplace=True)\n",
    "    DF.fillna(-1, inplace=True)\n",
    "    \n",
    "    DF['ncodpers'] = DF['ncodpers'].astype(np.int32)\n",
    "    DF['renta'] = DF['renta'].astype(np.float32)\n",
    "    DF['indrel'] = DF['indrel'].map(lambda x: 2 if x == 99 else x).astype(np.int8)\n",
    "\n",
    "    DF['ind_empleado'] = DF['ind_empleado'].map(lambda x: emp_dict[x]).astype(np.int8)\n",
    "\n",
    "    DF['sexo'] = DF['sexo'].map(lambda x: sexo_dict[x]).astype(np.int8)\n",
    "    DF['age'] = DF['age'].astype(np.int16)\n",
    "    DF['ind_nuevo'] = DF['ind_nuevo'].astype(np.int8)\n",
    "    DF['antiguedad'] = DF['antiguedad'].map(lambda x: -1 if x == '     NA' else x).astype(int)\n",
    "    DF['antiguedad'] = DF['antiguedad'].map(lambda x: -2 if x == -999999 else x).astype(np.int16)\n",
    "    DF['indrel_1mes'] = DF['indrel_1mes'].map(lambda x: -2 if x == 'P' else x).astype(np.float16)\n",
    "    DF['indrel_1mes'] = DF['indrel_1mes'].astype(np.int8)\n",
    "\n",
    "    DF['tiprel_1mes'] = DF['tiprel_1mes'].map(lambda x: tiprel_dict[x]).astype(np.int8)\n",
    "\n",
    "    DF['indresi'] = DF['indresi'].map(lambda x: indresi_dict[x]).astype(np.int8)\n",
    "\n",
    "    DF['indext'] = DF['indext'].map(lambda x: indext_dict[x]).astype(np.int8)\n",
    "\n",
    "    DF['conyuemp'] = DF['conyuemp'].map(lambda x: conyuemp_dict[x]).astype(np.int8)\n",
    "\n",
    "    DF['canal_entrada'] = DF['canal_entrada'].map(lambda x: canal_dict[x]).astype(np.int16)\n",
    "\n",
    "\n",
    "    DF['indfall'] = DF['indfall'].map(lambda x: indfall_dict[x]).astype(np.int8)\n",
    "\n",
    "    DF['pais_residencia'] = DF['pais_residencia'].map(lambda x: pais_dict[x]).astype(np.int8)\n",
    "\n",
    "    DF['tipodom'] = DF['tipodom'].astype(np.int8)\n",
    "\n",
    "    DF['cod_prov'] = DF['cod_prov'].astype(np.int8)\n",
    "\n",
    "    DF.drop('nomprov',axis=1,inplace=True)\n",
    "\n",
    "    DF['ind_actividad_cliente'] = DF['ind_actividad_cliente'].astype(np.int8)\n",
    "\n",
    "    DF['fecha_dato_month'] = DF['fecha_dato'].map(lambda x: int(x[5:7])).astype(np.int8)\n",
    "    DF['fecha_dato_year'] = DF['fecha_dato'].map(lambda x: int(x[0:4]) - 2015).astype(np.int8)\n",
    "    DF['month_int'] = (DF['fecha_dato_month'] + 12 * DF['fecha_dato_year']).astype(np.int8)\n",
    "    DF.drop('fecha_dato',axis=1,inplace=True)\n",
    "\n",
    "    DF['fecha_alta'] = DF['fecha_alta'].map(lambda x: '2020-00-00' if x == -1 else x)\n",
    "    DF['fecha_alta_month'] = DF['fecha_alta'].map(lambda x: int(x[5:7])).astype(np.int8)\n",
    "    DF['fecha_alta_year'] = DF['fecha_alta'].map(lambda x: int(x[0:4]) - 2015).astype(np.int8)\n",
    "    DF['fecha_alta_day'] = DF['fecha_alta'].map(lambda x: int(x[8:10])).astype(np.int8)\n",
    "    DF['fecha_alta_motnh_int'] = (DF['fecha_alta_month'] + 12 * DF['fecha_dato_year']).astype(np.int8)\n",
    "    DF.drop('fecha_alta',axis=1,inplace=True)\n",
    "    DF['ult_fec_cli_1t'] = DF['ult_fec_cli_1t'].map(lambda x: '2020-00-00' if x == -1 else x)\n",
    "    DF['ult_fec_cli_1t_month'] = DF['ult_fec_cli_1t'].map(lambda x: int(x[5:7])).astype(np.int8)\n",
    "    DF['ult_fec_cli_1t_year'] = DF['ult_fec_cli_1t'].map(lambda x: int(x[0:4]) - 2015).astype(np.int8)\n",
    "    DF['ult_fec_cli_1t_day'] = DF['ult_fec_cli_1t'].map(lambda x: int(x[8:10])).astype(np.int8)\n",
    "    DF['ult_fec_cli_1t_motnh_int'] = (DF['ult_fec_cli_1t_month'] + 12 * DF['ult_fec_cli_1t_year']).astype(np.int8)\n",
    "    DF.drop('ult_fec_cli_1t',axis=1,inplace=True)\n",
    "\n",
    "    DF['segmento'] = DF['segmento'].map(lambda x: segmento_dict[x]).astype(np.int8)\n",
    "\n",
    "    for col in target_cols:\n",
    "        if is_DF:\n",
    "            DF[col] = DF[col].astype(np.int8)\n",
    "\n",
    "    return DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.concat([resize_data(chunk) for chunk in train_raw])\n",
    "test = pd.concat([resize_data(chunk, is_DF=False) for chunk in test_raw])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11247309, 55)\n",
      "(929615, 31)\n",
      "nb of variables in the train dataset\n",
      "55\n",
      "nb of variables in the test dataset\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "#print(train.head(10))\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "print(\"nb of variables in the train dataset\")\n",
    "print(train.columns.size)\n",
    "print(\"nb of variables in the test dataset\")\n",
    "print(test.columns.size)\n",
    "\n",
    "#print(train.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ncodpers  ind_empleado  pais_residencia  sexo  age  ind_nuevo  antiguedad  \\\n",
      "0    928100             0                0     1   24          0          47   \n",
      "1    928099             0                0     1   24          0          47   \n",
      "2    928107             0                0     1   24          0          47   \n",
      "3    928263             0                0     1   41          0          47   \n",
      "4    928167             0                0     1   26          0          47   \n",
      "\n",
      "   indrel  indrel_1mes  tiprel_1mes            ...             \\\n",
      "0       1            1            0            ...              \n",
      "1       1            1            1            ...              \n",
      "2       1            1            0            ...              \n",
      "3       1            1            1            ...              \n",
      "4       1            1            1            ...              \n",
      "\n",
      "   fecha_dato_year  month_int  fecha_alta_month  fecha_alta_year  \\\n",
      "0                0          4                 8               -4   \n",
      "1                0          4                 8               -4   \n",
      "2                0          4                 8               -4   \n",
      "3                0          4                 8               -4   \n",
      "4                0          4                 8               -4   \n",
      "\n",
      "   fecha_alta_day  fecha_alta_motnh_int  ult_fec_cli_1t_month  \\\n",
      "0               4                     8                     0   \n",
      "1               4                     8                     0   \n",
      "2               4                     8                     0   \n",
      "3               4                     8                     0   \n",
      "4               4                     8                     0   \n",
      "\n",
      "   ult_fec_cli_1t_year  ult_fec_cli_1t_day  ult_fec_cli_1t_motnh_int  \n",
      "0                    5                   0                        60  \n",
      "1                    5                   0                        60  \n",
      "2                    5                   0                        60  \n",
      "3                    5                   0                        60  \n",
      "4                    5                   0                        60  \n",
      "\n",
      "[5 rows x 55 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "print(train[\"fecha_dato_year\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def removeNullVarColumns(train, test):\n",
    "    remove = []\n",
    "    for col in train.columns:\n",
    "        if train[col].std() == 0:\n",
    "            remove.append(col)\n",
    "\n",
    "    print(\"colonnes constantes : \", remove)\n",
    "    test.drop(remove, axis=1, inplace=True)\n",
    "    train.drop(remove, axis=1, inplace=True)\n",
    "    \n",
    "    return (train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['indfall', 'conyuemp', 'ind_empleado', 'indext', 'tiprel_1mes', 'sexo', 'indresi', 'segmento'])\n",
      "{'indfall': array([ 0, -1,  1], dtype=int64), 'conyuemp': array([-1,  0,  1], dtype=int64), 'ind_empleado': array([ 0, -1,  1,  2,  3,  4], dtype=int64), 'indext': array([ 0,  1, -1], dtype=int64), 'tiprel_1mes': array([ 0,  1, -1,  2,  4,  3], dtype=int64), 'sexo': array([ 1,  0, -1], dtype=int64), 'indresi': array([ 1, -1,  0], dtype=int64), 'segmento': array([ 2,  3, -1,  1], dtype=int64)}\n"
     ]
    }
   ],
   "source": [
    "var_q = [\"ind_empleado\", \"indfall\", \"sexo\", \"tiprel_1mes\", \"indresi\", \"indext\", \"conyuemp\", \"segmento\"]\n",
    "unique_var_train = {var:train[var].unique() for var in var_q}\n",
    "unique_var_test = {var:test[var].unique() for var in var_q}\n",
    "print(unique_var_test.keys())\n",
    "print(unique_var_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "#On crée des variables binaires à partir des variables multi-catégoriques pour le dataset de train \n",
    "for var in unique_var_train.keys(): \n",
    "    lb = LabelBinarizer()\n",
    "    tempTrain = lb.fit_transform(train[var])\n",
    "    for i in range(tempTrain[0,:].size):\n",
    "        train[var+\"_\"+str(unique_var_train[var][i])] = tempTrain[:,i].astype(np.int8)\n",
    "\n",
    "#On fait de même avec le dataset de test\n",
    "for var in unique_var_test.keys(): \n",
    "    lb2 = LabelBinarizer()\n",
    "    tempTest = lb2.fit_transform(test[var])\n",
    "    for j in range(tempTest[0,:].size):\n",
    "        test[var+\"_\"+str(unique_var_test[var][j])] = tempTest[:,j].astype(np.int8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns that are in train but not in test : \n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "#Pour les variables dans le datasest train mais qui ne sont pas dans le dataset test (on exclut les variables cibles)\n",
    "#On les ajoute dans notre dataset de test avec des valeurs à 0 \n",
    "\n",
    "print(\"columns that are in train but not in test : \")\n",
    "col_to_add = [col for col in train.columns if col not in test.columns and col not in target_cols]\n",
    "print(col_to_add)\n",
    "for col in col_to_add:\n",
    "    test[col] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test = removeNullVarColumns(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4  6  7  8  9 10 11 12  1  2  3]\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "train_val = train[(train[\"fecha_dato_year\"]==1) & (train[\"fecha_dato_month\"]==5)]\n",
    "train_train = train[((train[\"fecha_alta_year\"]!=1) & (train[\"fecha_dato_month\"]!=5))]\n",
    "\n",
    "print(train_train[\"fecha_dato_month\"].unique())\n",
    "print(train_train[\"fecha_dato_year\"].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 13647309 entries, 0 to 47308\n",
      "Data columns (total 86 columns):\n",
      "ncodpers                    int32\n",
      "ind_empleado                int8\n",
      "pais_residencia             int8\n",
      "sexo                        int8\n",
      "age                         int16\n",
      "ind_nuevo                   int8\n",
      "antiguedad                  int16\n",
      "indrel                      int8\n",
      "indrel_1mes                 int8\n",
      "tiprel_1mes                 int8\n",
      "indresi                     int8\n",
      "indext                      int8\n",
      "conyuemp                    int8\n",
      "canal_entrada               int16\n",
      "indfall                     int8\n",
      "tipodom                     int8\n",
      "cod_prov                    int8\n",
      "ind_actividad_cliente       int8\n",
      "renta                       float32\n",
      "segmento                    int8\n",
      "ind_ahor_fin_ult1           int8\n",
      "ind_aval_fin_ult1           int8\n",
      "ind_cco_fin_ult1            int8\n",
      "ind_cder_fin_ult1           int8\n",
      "ind_cno_fin_ult1            int8\n",
      "ind_ctju_fin_ult1           int8\n",
      "ind_ctma_fin_ult1           int8\n",
      "ind_ctop_fin_ult1           int8\n",
      "ind_ctpp_fin_ult1           int8\n",
      "ind_deco_fin_ult1           int8\n",
      "ind_deme_fin_ult1           int8\n",
      "ind_dela_fin_ult1           int8\n",
      "ind_ecue_fin_ult1           int8\n",
      "ind_fond_fin_ult1           int8\n",
      "ind_hip_fin_ult1            int8\n",
      "ind_plan_fin_ult1           int8\n",
      "ind_pres_fin_ult1           int8\n",
      "ind_reca_fin_ult1           int8\n",
      "ind_tjcr_fin_ult1           int8\n",
      "ind_valo_fin_ult1           int8\n",
      "ind_viv_fin_ult1            int8\n",
      "ind_nomina_ult1             int8\n",
      "ind_nom_pens_ult1           int8\n",
      "ind_recibo_ult1             int8\n",
      "fecha_dato_month            int8\n",
      "fecha_dato_year             int8\n",
      "month_int                   int8\n",
      "fecha_alta_month            int8\n",
      "fecha_alta_year             int8\n",
      "fecha_alta_day              int8\n",
      "fecha_alta_motnh_int        int8\n",
      "ult_fec_cli_1t_month        int8\n",
      "ult_fec_cli_1t_year         int8\n",
      "ult_fec_cli_1t_day          int8\n",
      "ult_fec_cli_1t_motnh_int    int8\n",
      "indfall_0                   int8\n",
      "indfall_-1                  int8\n",
      "indfall_1                   int8\n",
      "conyuemp_-1                 int8\n",
      "conyuemp_0                  int8\n",
      "conyuemp_1                  int8\n",
      "ind_empleado_0              int8\n",
      "ind_empleado_-1             int8\n",
      "ind_empleado_1              int8\n",
      "ind_empleado_2              int8\n",
      "ind_empleado_3              int8\n",
      "ind_empleado_4              int8\n",
      "indext_0                    int8\n",
      "indext_1                    int8\n",
      "indext_-1                   int8\n",
      "tiprel_1mes_0               int8\n",
      "tiprel_1mes_1               int8\n",
      "tiprel_1mes_-1              int8\n",
      "tiprel_1mes_2               int8\n",
      "tiprel_1mes_4               int8\n",
      "tiprel_1mes_3               int8\n",
      "sexo_1                      int8\n",
      "sexo_0                      int8\n",
      "sexo_-1                     int8\n",
      "indresi_1                   int8\n",
      "indresi_-1                  int8\n",
      "indresi_0                   int8\n",
      "segmento_2                  int8\n",
      "segmento_3                  int8\n",
      "segmento_-1                 int8\n",
      "segmento_1                  int8\n",
      "dtypes: float32(1), int16(3), int32(1), int8(81)\n",
      "memory usage: 1.3 GB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(train.info())"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
